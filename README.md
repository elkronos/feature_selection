# Feature Selection Repository

This repository contains a collection of functions to aid in feature selection and dimensionality reduction. Each script comprises a function along with detailed documentation explaining its purpose, usage examples, and other pertinent information. All functions are implemented in R.

## Function Overview

| Function Name          | Description                                                                                                           | Key Assumptions                                                                                                 | Outputs                                                                                              | Primary Applications                                                                         | When to Use                                                                                              |
|------------------------|-----------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
| `fs_bayes`             | Bayesian model-based feature selection for model optimization.                                                        | Data contains necessary columns (response, predictors, optional date), data preprocessing steps are successful, and the Bayesian modeling assumptions (e.g., priors, model structure) are appropriate. | Best model, updated data with fitted values and residuals, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE). | Identifying crucial predictors, optimizing model performance, evaluating model fit, and making informed decisions based on Bayesian inference. | When you need robust model evaluation with uncertainty estimates and can afford the computational cost of Bayesian methods. |
| `fs_boruta`            | Performs feature selection from a dataset using the Boruta algorithm, with options to limit features and remove highly correlated variables. | Target variable exists in the dataset, appropriate handling of parallel processing, and consideration of correlation among features.                  | Selected features, Boruta result object.                                                            | Selecting important features for predictive modeling, improving model interpretability, and reducing overfitting by removing irrelevant features. | When you need a straightforward feature selection method that is robust against overfitting and can handle various types of data. |
| `fs_chi`               | Evaluates the association between categorical features and a target variable using the chi-square test, handling missing values and low frequencies. | Target variable and categorical features exist in the dataset, and chi-square test assumptions are met.                                             | Significant features, p-values from chi-square tests.                                                | Identifying significant categorical features for classification tasks, especially when dealing with categorical data. | When you have categorical data and need to assess feature significance based on association with the target variable. |
| `fs_correlation`       | Selects features from a dataset based on their correlation with other features, providing a correlation matrix and the names of selected variables. | Data is numeric and appropriately preprocessed, specified correlation method is valid, and threshold is between 0 and 1.                              | Correlation matrix, selected variable names.                                                         | Reducing dimensionality by selecting features based on correlation, identifying multicollinearity, and improving model performance by removing highly correlated features. | When you need to manage multicollinearity in a dataset by selecting or removing highly correlated features. |
| `fs_elastic`           | Performs feature selection and model training using elastic net regularization, optionally applying PCA.              | Data contains numeric predictor and response variables, proper handling of missing values, and valid hyperparameter tuning grid.                      | Coefficients of the best model, alpha, lambda, RMSE value.                                           | Combining feature selection and model training for regression tasks, handling multicollinearity, and leveraging both Lasso and Ridge penalties. | When you need an efficient and robust method for both feature selection and model training, especially with high-dimensional data. |
| `fs_infogain`          | Calculates information gain for each variable in a data frame with respect to a target variable.                       | Target variable exists in the dataset, numeric and categorical data are handled appropriately.                                                       | Data frame with variables and their corresponding information gains.                                  | Identifying important features for predictive modeling by measuring their contribution to reducing uncertainty. | When you need to evaluate the relevance of variables based on their information gain with respect to the target variable. |
| `fs_lasso`             | Fits a Lasso regression model to the provided data using cross-validation, extracts variable importance scores, and optionally returns the fitted model object. | Data contains numeric predictor and response variables, proper handling of missing values, and valid hyperparameter tuning grid.                      | Variable importance scores, optionally the trained Lasso model.                                      | Selecting important features for regression tasks, handling multicollinearity, and using Lasso penalty for variable selection. | When you need a regularization method that performs both variable selection and regularization to enhance prediction accuracy and interpretability. |
| `fs_mars`              | Performs feature selection and model training using Multivariate Adaptive Regression Splines (MARS).                  | Required libraries are loaded, response column exists, proper handling of missing values, balanced classes, valid hyperparameter tuning grid.         | Trained MARS model, performance metrics (RMSE for numeric response, accuracy for factor response).    | Handling non-linear relationships and interactions between features for regression tasks.                 | When you need to model complex, non-linear relationships in your data and require an interpretable model that can automatically select important features. |
| `fs_pca`               | Performs PCA on the numeric columns of the data, optionally plots the results if a label column is specified.           | Data contains numeric columns, valid PCA assumptions are met, and the number of principal components to retain is specified.                          | PCA loadings, scores, variance explained, data table of PCA results, and optional PCA plot.           | Reducing dimensionality, visualizing data in lower dimensions, and identifying principal components for further analysis. | When you need to reduce the dimensionality of your data, visualize patterns, or identify the most significant components driving variability in your dataset. |
| `fs_randomforest`      | Applies the random forest algorithm for classification or regression tasks with options for feature selection, preprocessing, and parallel computing. | Target variable exists, specified type is either "classification" or "regression", valid preprocessing and feature selection functions if provided.   | Trained random forest model, predictions for the target variable, accuracy for classification or RMSE for regression. | Building robust predictive models, assessing feature importance, and handling large datasets with parallel processing. | When you need a versatile and powerful ensemble method for classification or regression that can handle large datasets and provide feature importance insights. |
| `fs_recursivefeature`  | Performs Recursive Feature Elimination (RFE) to select optimal features and trains the final model using the optimal set of features. | Data is preprocessed properly, control parameters for RFE and model training are valid, and appropriate feature functions are specified.              | Optimal number of variables, optimal variables, variable importance, resampling results, optionally the final trained model. | Selecting the most important features for model training using RFE, improving model interpretability and performance. | When you need to identify and select the most important features in your dataset through a robust feature elimination process. |
| `fs_stepwise`          | Performs stepwise feature selection using nested cross-validation, trains the final model, and evaluates variable importance. | Dependent variable exists in the dataset, valid stepwise method is specified, appropriate control parameters for cross-validation.                    | Results, best tuning parameters, final model, variable importance.                                   | Selecting important features using stepwise regression, improving model performance through feature selection, and cross-validating the final model. | When you need a systematic method for feature selection that evaluates the contribution of each feature and ensures model robustness through cross-validation. |
| `fs_svd`               | Performs Singular Value Decomposition (SVD) on a matrix, optionally scaling and centering the input data.               | Input is a valid numeric matrix, no missing values in the input matrix, valid scaling option specified.                                               | Singular values, left singular vectors, right singular vectors.                                       | Reducing dimensionality, uncovering latent structures in the data, and performing data compression.         | When you need to decompose a matrix into its singular values and vectors for dimensionality reduction, noise reduction, or to analyze the underlying structure of the data. |
| `fs_svm`               | Performs SVM classification or regression with optional feature selection and class imbalance handling.                 | Data is a valid data frame, target variable exists, task is either "classification" or "regression", train_ratio is between 0 and 1, number of folds for cross-validation is greater than 1. | Trained SVM model, test set, predictions, performance metrics (accuracy for classification, R-squared, MSE, MAE for regression). | Building SVM models for classification or regression, handling class imbalance, performing feature selection. | When you need a powerful and flexible model for classification or regression tasks, and want to incorporate feature selection and handle class imbalance. |
| `fs_variance`          | Applies variance thresholding to a numeric dataset, removing features whose variances are not above a certain threshold. | Data is a numeric matrix or data frame, threshold is a non-negative numeric value, proper handling of parallel processing.                             | Numeric matrix containing the thresholded data, or NULL if no features meet the threshold.             | Reducing dimensionality by removing features with low variance, improving model performance by eliminating irrelevant features. | When you need to reduce the number of features in your dataset by removing those with low variance, especially in high-dimensional data. |
                                          |

## Descriptions
Below is a more detailed description of each function.

### 1. `fs_bayes` 
Performs feature selection for Bayesian models using the `brms` package in R. It generates all possible combinations of predictor columns, fits a Bayesian model for each combination, and selects the best model based on the lowest WAIC value. The function also calculates model quality metrics such as MAE and RMSE. This function is useful when you want to identify the most relevant predictors for your Bayesian model and evaluate their performance.

### 2. `fs_boruta`
Used to select features using the Boruta algorithm. It removes the target variable from the data and then runs the Boruta algorithm to identify relevant features. It also allows for limiting the number of selected features and removing highly correlated variables. It is especially useful when dealing with high-dimensional datasets where you want to automate the feature selection process.

### 3. `fs_chi`
Performs feature selection on a data frame by applying the chi-square test to identify categorical features that are statistically significant with respect to the target variable. It returns a list containing the names of the significant categorical features and their corresponding p-values. This function can be used when you want to determine which categorical features have a significant association with the target variable in order to reduce the dimensionality of the dataset or improve the performance of a predictive model.

### 4. `fs_correlation`
Selects features based on their correlation with other features. It calculates the correlation matrix using either Pearson or Spearman correlation method, finds the variables that have a correlation above a specified threshold, and returns a list containing the correlation matrix and the selected variables. It can be used when you want to identify and select features that are highly correlated with each other in a dataset.

### 5. `fs_elastic` 
Performs elastic net regression with cross-validation using the `glmnet` package. It takes a data frame with response and predictor variables, a formula specifying the model, a numeric vector for the mixing parameter (alpha), a `trainControl` object for cross-validation settings, and a logical value indicating whether to perform principal component analysis (PCA) on the predictors. It returns a list with coefficients, alpha value, and lambda value for the best model. This function is useful when you want to perform elastic net regression with cross-validation to select the best model and assess the importance of predictors in the presence of multicollinearity.

### 6. `fs_infogain`
Calculates the information gain for each variable in a given data frame, which measures the reduction in entropy or uncertainty when the data is split based on that variable. It is used in feature selection tasks to identify the variables that are most informative or relevant for predicting the target variable. The function also handles date columns by extracting year, month, and day features. By calculating the information gain for each variable, you can prioritize and select the variables that contribute the most to the predictive power of your model.

### 7. `fs_lasso` 
Trains and tests a Lasso regression model on a dataset. It fits the model using cross-validation and calculates variable importance scores. It is useful when you want to understand the importance of different predictor variables in predicting a response variable and want to perform feature selection.

### 8. `fs_mars` 
Used to train and evaluate a MARS (Multivariate Adaptive Regression Splines) model on a given dataset. It splits the dataset into training and test sets, performs grid search over a predefined set of hyperparameters, trains the MARS model on the training set using cross-validation, and then evaluates the model by making predictions on the test set. It calculates the root mean squared error (RMSE) for numeric response variables or accuracy for categorical response variables and returns the evaluation metric along with the trained model. This function can be used when you want to build and assess a MARS model for regression or classification tasks.

### 9. `fs_pca` 
Performs principal component analysis (PCA) on a given dataset. It calculates the principal component loadings, scores, and the proportion of variance explained by each principal component. It also generates a data table with the principal component scores and labels. This function can be used when you want to reduce the dimensionality of your data and analyze the underlying patterns or relationships among variables. The function also provides a PCA plot to visualize the data in the reduced-dimensional space.

### 10. `fs_randomforest`
Used to apply the random forest algorithm for classification or regression tasks. It takes in a data frame with features and a target variable, and performs various operations such as data preprocessing, feature selection, and parallel computing. It returns a list containing the trained random forest model, predictions for the target variable, and the accuracy (classification accuracy or RMSE) of the model.

### 11. `fs_recursivefeature` 
Loads a specified dataset, splits it into training and testing sets, and performs Recursive Feature Elimination (RFE) using the Random Forest algorithm on the training set. It returns a data frame containing the variable importance scores computed from the RFE analysis. This function can be used when you want to identify the most important variables in a dataset for predicting a specific outcome or target variable.

### 12. `fs_stepwise` 
Performs stepwise regression with cross-validation. It takes a data frame, the name of the dependent variable, and the type of stepwise selection (backward, forward, or both) as inputs. It returns a list containing the results of the cross-validation, the best tuning parameter value, a summary of the final model, and the coefficients of the final model. This function is useful when you want to perform feature selection and evaluate the performance of the regression model using cross-validation.

### 13. `fs_svd` 
Performs Singular Value Decomposition (SVD) on a matrix. It takes the input matrix, an optional argument to scale the matrix, and an optional argument to specify the number of singular values to keep. One might use this function to analyze the structure and properties of a matrix, such as reducing dimensionality, identifying dominant features, or solving linear systems of equations.

### 14. `fs_svm` 
Trains an SVM (Support Vector Machine) model using cross-validation and grid search. It can be used for classification or regression tasks. It takes a data frame, the target variable, the type of task, the number of cross-validation folds, and an optional tuning grid as inputs. It returns a list containing the trained model, predictions, and either the accuracy (for classification) or the R-squared score (for regression).

### 15. `fs_variance`
Takes a numeric matrix or data frame as input and applies variance thresholding to remove features (columns) whose variances are below a specified threshold. It returns a numeric matrix with the thresholded data. One might use this function to preprocess data before further analysis or modeling, especially when dealing with high-dimensional datasets and wanting to focus on the most informative features.
